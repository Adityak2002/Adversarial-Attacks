# AI Model Security Against Adversarial Attacks

## Overview
This project focuses on building robust AI models resistant to adversarial attacks, which are maliciously crafted inputs designed to mislead AI systems. The approach achieves high accuracy while maintaining strong defenses against adversarial perturbations.

## Features
- Implements effective strategies to counter adversarial examples.
- High model accuracy with improved robustness.
- Supports various attack scenarios like FGSM and PGD.

## Tech Stack
- **Programming Language:** Python
- **Libraries:** PyTorch, NumPy, Matplotlib
